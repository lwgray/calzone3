{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from feature_extraction import Blob, Words\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from time import time\n",
    "from pprint import pprint\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "from gp_test import main\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = main(sub='python', start='01/17/2018', end='01/24/2018')\n",
    "data.title = data.title.astype('str')\n",
    "#data = next(data_gen)\n",
    "#data = pd.read_csv('processed_posts.csv.bz2')\n",
    "#data = pd.read_csv('processed_python.csv.bz2', encoding='ISO-8859-1', compression='bz2')\n",
    "data = pd.read_csv('processed_datascience.csv.bz2')\n",
    "data.drop_duplicates('title', inplace=True)\n",
    "data = data[np.abs(data.ups-data.ups.mean())<=(2*data.ups.std())]\n",
    "data['logups'] = np.log1p(data['ups'])\n",
    "\n",
    "#data = data[data['subreddit'] == 'r/books']\n",
    "train_X, test_X, train_y, test_y = train_test_split(data.title, \n",
    "                                                    data.logups, \n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.3170731707317076"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.ups.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "                      \n",
    "            ('words', Words()),\n",
    "                      \n",
    "            ('title', Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,3), sublinear_tf=True, stop_words='english')),\n",
    "                ('svd', TruncatedSVD(n_components=120)),\n",
    "                ('normalize', MinMaxScaler(copy=False)),\n",
    "                ('selector', SelectPercentile(f_classif, percentile=10))\n",
    "            ])),\n",
    "            \n",
    "            ('blob', Pipeline([\n",
    "                ('all', Blob()),\n",
    "                ('minmax', MinMaxScaler()),\n",
    "            ])),\n",
    "            \n",
    "            ])),\n",
    "    ('clf', Ridge()),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('union', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('words', Words()), ('title', Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=T...it_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001))])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.630013960986278"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pipeline.predict(test_X)\n",
    "mean_absolute_error(y_pred=np.expm1(y), y_true=np.expm1(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.1857091209382684"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 2\n",
    "answer = pipeline.predict(pd.Series(['A Gentle Introduction to N-Dimensional Arrays in Python with NumPy - Machine Learning Mastery']))[0]\n",
    "np.expm1(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
