{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('processed_datascience.csv.bz2')\n",
    "import numpy as np\n",
    "from calzone import describe\n",
    "from feature_extraction import Blob\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from textblob import TextBlob\n",
    "from textblob.taggers import NLTKTagger, PatternTagger\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from nltk.corpus import cmudict\n",
    "from textstat.textstat import textstat\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from feature_extraction import Blob, Words, Exclude, WordCount, POS, Readable\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "data = pd.read_csv('processed_datascience.csv.bz2')\n",
    "data['z_scores'] = np.abs((data.ups-data.ups.mean())/data.ups.std())\n",
    "data = data[data['z_scores']<= 2.5]\n",
    "\n",
    "## Optional: Log transformation of up-votes\n",
    "data['log_ups'] = np.log1p(data['ups'])\n",
    "\n",
    "# Create Label column defining whether or not the article's upvotes exceed the average vote for the subreddit\n",
    "data['gtavg'] = data.log_ups > data.log_ups.mean()\n",
    "train_X, test_X, train_y, test_y = train_test_split(data.title, \n",
    "                                                    data.gtavg, \n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "index = 3\n",
    "def lemmatize(token, tag):\n",
    "    if tag[0].lower() in ['n', 'v']:\n",
    "        return lemmatizer.lemmatize(token, tag[0].lower())\n",
    "    return token\n",
    "tagged_corpus = [pos_tag(word_tokenize(data['title'][index].replace(\"/\",\" \" )))]\n",
    "lc = [lemmatize(token, tag) for token, tag in tagged_corpus[0]]\n",
    "tc = [stemmer.stem(token) for token in word_tokenize(data.title[index].replace(\"/\", \" \"))]\n",
    "print(data.title[index])\n",
    "print(tc)\n",
    "print(lc)\n",
    "# lemma = [[lemmatize(token, tag) for token, tag in document ] for document in tagged_corpus]\n",
    "# [lemmatizer.lemmatize(token) for token, _ in tc]\n",
    "#lemmatize('analytics', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(data):\n",
    "    def lemma(token, tag):\n",
    "        if tag[0].lower() in ['n', 'v']:\n",
    "            return lemmatizer.lemmatize(token, tag[0].lower())\n",
    "        return token\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tagged_corpus = [pos_tag(word_tokenize(title)) for title in data['title'].str.replace(\"/\", \" \").str.lower()]\n",
    "    lc = [\" \".join([lemma(token, tag) for token, tag in title]) for title in tagged_corpus]\n",
    "    return lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = lemmatize(data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b', analyzer='word', ngram_range=(1,3), stop_words='english', strip_accents='unicode').fit(train_X, train_y)\n",
    "train_titles_tfidf = tfidf.transform(train_X)\n",
    "test_titles_tfidf = tfidf.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=120)\n",
    "svd.fit(train_titles_tfidf)\n",
    "svd.get_params()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_titles_tfidf = tfidf.transform(train_X)\n",
    "#test_titles_tfidf = tfidf.transform(test_X)\n",
    "\n",
    "# reg = GradientBoostingClassifier()\n",
    "# reg.fit(train_titles_tfidf, train_labels)\n",
    "# print(reg.score(test_titles_tfidf, test_labels))\n",
    "# train_predictions = reg.predict(train_titles_tfidf)\n",
    "# test_predictions = reg.predict(test_titles_tfidf)\n",
    "# print(mean_absolute_error(test_predictions, test_labels))\n",
    "\n",
    "# # print(train_set.iloc[0])\n",
    "# print(train_predictions[3], train_labels.iloc[3])\n",
    "# plt.scatter(train_labels, train_predictions)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "                      \n",
    "            ('pipe', Pipeline([\n",
    "                ('inner', FeatureUnion(\n",
    "                    transformer_list=[\n",
    "                        ('pos', POS()),\n",
    "\n",
    "                        ('read', Readable()),\n",
    "\n",
    "                        ('words', Words()),\n",
    "\n",
    "                        ('blob', Pipeline([\n",
    "                            ('all', Blob()),\n",
    "                            ('minmax', MinMaxScaler()),\n",
    "                        ])),\n",
    "                ])),\n",
    "                ('select', SelectFromModel(ExtraTreesClassifier()))\n",
    "                     \n",
    "            ])),\n",
    "                      \n",
    "            ('title', Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b', ngram_range=(1,3), sublinear_tf=True, stop_words='english')),\n",
    "                ('svd', TruncatedSVD(n_components=120)),\n",
    "                ('normalize', MinMaxScaler(copy=False)),\n",
    "                ('selector', SelectPercentile(f_classif, percentile=10))\n",
    "            ])),\n",
    "\n",
    "            \n",
    "            ])),\n",
    "    ('clf', RandomForestClassifier(n_estimators=190, n_jobs=-1, max_depth=5, max_features='log2',\n",
    "                                   min_samples_leaf=1, min_samples_split=77)),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = fu.fit_transform(train_X)\n",
    "b = fu.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ExtraTreesClassifier()\n",
    "clf = clf.fit(a, train_y)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.feature_importances_\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_new = model.transform(a)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "                      \n",
    "            ('pos', POS()),\n",
    "            \n",
    "            ('read', Readable()),\n",
    "            \n",
    "            ('words', Words()),\n",
    "                                \n",
    "            ('blob', Pipeline([\n",
    "                ('all', Blob()),\n",
    "                ('minmax', MinMaxScaler()),\n",
    "            ])),\n",
    "            \n",
    "            ])),\n",
    "    ('select', SelectFromModel(ExtraTreesClassifier())),\n",
    "        ])\n",
    "\n",
    "# Train model\n",
    "a = pipeline.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fu = FeatureUnion(\n",
    "        transformer_list=[\n",
    "            ('pos', POS()),\n",
    "\n",
    "            ('read', Readable()),\n",
    "\n",
    "            ('words', Words()),\n",
    "\n",
    "            ('blob', Pipeline([\n",
    "                ('all', Blob()),\n",
    "                ('minmax', MinMaxScaler()),\n",
    "            ])),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fu.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = fu.transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Words()\n",
    "y = x.transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
