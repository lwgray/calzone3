{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Model Visual Evaluation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "import pandas as pd\n",
    "from yellowbrick.classifier import ClassificationReport, ConfusionMatrix, ROCAUC, ClassBalance\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Import previously created model see classify.ipynb\n",
    "# Read in data then split data into training and test sets\n",
    "# Create Label column defining whether or not the article's upvotes exceed the average vote for the subreddit\n",
    "# Split data into training and test sets\n",
    "\n",
    "pipeline = joblib.load('datascience.pkl')\n",
    "data = pd.read_csv('processed_datascience.csv.bz2')\n",
    "\n",
    "data['z_scores'] = np.abs((data.ups-data.ups.mean())/data.ups.std())\n",
    "data = data[data['z_scores']<= 2.5]\n",
    "\n",
    "## Optional: Log transformation of up-votes\n",
    "data['log_ups'] = np.log1p(data['ups'])\n",
    "\n",
    "# Create Label column defining whether or not the article's upvotes exceed the average vote for the subreddit\n",
    "data['gtavg'] = data.log_ups > data.log_ups.mean()\n",
    "\n",
    "# Change Target label to Success or Failure\n",
    "booldict = {True: 'Success', False: 'Failure'}\n",
    "data['gtavg'] = data.gtavg.map(booldict)\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(data.title, \n",
    "                                                    data.gtavg, \n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = ClassificationReport(pipeline)\n",
    "visualizer.fit(train_X, train_y)\n",
    "visualizer.score(test_X,test_y)\n",
    "visualizer.poof('clsreport.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ConfusionMatrix(pipeline)\n",
    "cm.fit(train_X, train_y)\n",
    "cm.score(test_X, test_y)\n",
    "cm.poof('confusion.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = ROCAUC(pipeline)\n",
    "visualizer.fit(train_X, train_y)\n",
    "visualizer.score(test_X, test_y)\n",
    "g=visualizer.poof('rocauc.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = ClassBalance(pipeline)\n",
    "visualizer.fit(train_X, train_y)\n",
    "visualizer.score(test_X, test_y)\n",
    "g=visualizer.poof('classbalance.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
